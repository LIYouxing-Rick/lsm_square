from src.network import *
import torch
import torch.nn as nn
import warnings
__all__ = ['Newmodel', 'get_model']

import torch
import torch.nn as nn
import geoopt


class MultiPrototypeHyperbolicClassifier(nn.Module):
    """
    Multi-prototype Hyperbolic RMLR Classifier

    - æ¯ä¸ªç±»åˆ« K ä¸ªè¶…æ›²ä¸­å¿ƒï¼ˆprototypeï¼‰
    - ä½¿ç”¨ log-sum-exp èšåˆ
    - ä¸åŸ HyperbolicClassifier forward æ¥å£å®Œå…¨ä¸€è‡´
    """

    def __init__(
        self,
        in_dim: int,
        num_classes: int,
        num_prototypes: int = 4,   # ğŸ‘ˆ K
        init_gamma: float = 1.0,
        init_g: float = 1.0,
    ):
        super().__init__()

        self.in_dim = in_dim
        self.num_classes = num_classes
        self.num_prototypes = num_prototypes

        # PoincarÃ© ball
        self.manifold = geoopt.manifolds.PoincareBall(c=1.0)

        # ---- å¤š prototype è¶…æ›²ä¸­å¿ƒ ----
        # shape: [C, K, D]
        self.weight_v = geoopt.ManifoldParameter(
            self.manifold.random((num_classes, num_prototypes, in_dim)),
            manifold=self.manifold
        )

        # æ¯ä¸ªç±»åˆ«ä¸€ä¸ª scaleï¼ˆä¸åŸå®ç°ä¸€è‡´ï¼‰
        self.weight_g = nn.Parameter(
            torch.ones(num_classes) * init_g
        )

        # margin / radius
        self.gamma = nn.Parameter(
            torch.tensor(init_gamma)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [B, in_dim]   (PoincarÃ© ball)
        return: [B, num_classes]
        """
        # ---- æ›²ç‡ ----
        c = torch.as_tensor(
            self.manifold.c,
            device=x.device,
            dtype=x.dtype
        )
        rc = torch.sqrt(c)

        # ---- prototypes ----
        # z: [C, K, D]
        z = self.weight_v

        # æ­£ç¡®çš„æ–¹å‘å½’ä¸€åŒ–ï¼ˆæœ€åä¸€ç»´ï¼‰
        z_norm = z.norm(dim=-1, keepdim=True).clamp_min(1e-15)
        z_unit = z / z_norm

        # ---- è¾“å…¥ ----
        rcx = rc * x                           # [B, D]
        cx2 = rcx.pow(2).sum(dim=-1, keepdim=True)  # [B, 1]

        # ---- inner product ----
        # [B, D] Â· [C, K, D] â†’ [B, C, K]
        inner = torch.einsum("bd,ckd->bck", rcx, z_unit)

        # ---- RMLR ----
        drcr = 2.0 * rc * self.gamma

        num = (
            2.0 * inner * torch.cosh(drcr)
            - (1.0 + cx2.unsqueeze(-1)) * torch.sinh(drcr)
        )
        den = torch.clamp(1.0 - cx2, min=1e-15)

        logits_ck = (
            2.0 * self.weight_g.view(1, -1, 1) / rc
            * torch.asinh(num / den.unsqueeze(-1))
        )  # [B, C, K]

        # ---- prototype èšåˆï¼ˆå…³é”®ä¸€æ­¥ï¼‰----
        # æ¨èï¼šlog-sum-expï¼ˆç¨³å®š + è¡¨è¾¾åŠ›å¼ºï¼‰
        logits = torch.logsumexp(logits_ck, dim=-1)  # [B, C]

        return logits


class Newmodel(Basemodel):
    """replace the image representation method and classifier

       Args:
       modeltype: model archtecture
       representation: image representation method
       num_classes: the number of classes
       freezed_layer: the end of freezed layers in network
       pretrained: whether use pretrained weights or not
    """
    def __init__(self, modeltype, representation, num_classes, freezed_layer, pretrained=False):
        super(Newmodel, self).__init__(modeltype, pretrained)
        if representation is not None:
            representation_method = representation['function']
            representation.pop('function')
            representation_args = representation
            representation_args['input_dim'] = self.representation_dim
            self.representation = representation_method(**representation_args)
            fc_input_dim = self.representation.output_dim
            if not pretrained:
                if isinstance(self.classifier, nn.Sequential): # for alexnet and vgg*
                    conv6_index = 0
                    for m in self.classifier.children():
                        if isinstance(m, nn.Linear):
                            output_dim = m.weight.size(0) # 4096
                            self.classifier[conv6_index] = nn.Linear(fc_input_dim, output_dim)
                            break
                        conv6_index += 1
                    if representation_args.get('corr_method', None) == 'phcm':
                        self.classifier[-1] = HyperbolicClassifier(output_dim, num_classes)
                    else:
                        self.classifier[-1] = nn.Linear(output_dim, num_classes)
                else:
                    if representation_args.get('corr_method', None) == 'phcm':
                        self.classifier = MultiPrototypeHyperbolicClassifier(
    fc_input_dim,
    num_classes,
    num_prototypes=2   # ğŸ‘ˆ å»ºè®® 2 / 4 / 8 è¯•
)

                    else:
                        self.classifier = nn.Linear(fc_input_dim, num_classes)
            else:
                if representation_args.get('corr_method', None) == 'phcm':
                   self.classifier = MultiPrototypeHyperbolicClassifier(
    fc_input_dim,
    num_classes,
    num_prototypes=2   # ğŸ‘ˆ å»ºè®® 2 / 4 / 8 è¯•
)

                else:
                    self.classifier = nn.Linear(fc_input_dim, num_classes)
        else:
            if modeltype.startswith('alexnet') or modeltype.startswith('vgg'):
                output_dim = self.classifier[-1].weight.size(1) # 4096
                self.classifier[-1] = nn.Linear(output_dim, num_classes)
            else:
                self.classifier = nn.Linear(self.representation_dim, num_classes)
        index_before_freezed_layer = 0
        if freezed_layer:
            for m in self.features.children():
                if index_before_freezed_layer < freezed_layer:
                    m = self._freeze(m)
                index_before_freezed_layer += 1

    def _freeze(self, modules):
        for param in modules.parameters():
            param.requires_grad = False
        return modules


def get_model(modeltype, representation, num_classes, freezed_layer, pretrained=False):
    _model = Newmodel(modeltype, representation, num_classes, freezed_layer, pretrained=pretrained)
    return _model
